CrammedBert, a variant of BERT, modifies the standard architecture to strike a balance between computational efficiency and performance. It utilizes a more compact structure with fewer parameters, reducing the computational load while maintaining a high level of accuracy.

**UltraFastBert: Neuron Efficiency and Computational Architecture**

UltraFastBert stands out for its speed and efficiency. This model is designed to optimize neuron usage, enhancing its computational efficiency. The neuron efficiency in UltraFastBert can be quantified as:

$$ Neuron Efficiency= (Number of Active Neurons / Total Neurons) $$

This efficiency metric is crucial in large-scale NLP tasks where processing speed is as important as accuracy. UltraFastBert achieves this by streamlining the network architecture and reducing redundancy in neuron activation.

**Hyperparameter Settings for Reproducibility**

To ensure the reproducibility of our experiments, we meticulously documented the hyperparameter settings for each model:

-   **BERT**: Learning rate of 2×10−52×10−5, batch size of 32.
-   **CrammedBert**: Learning rate of 1×10−51×10−5, batch size of 16.
-   **UltraFastBert**: Learning rate of 3×10−53×10−5, batch size of 64.

These settings were optimized through a series of experiments to balance training efficiency and model performance.

**CUDA and PyTorch Compatibility**

Our project leveraged CUDA for GPU optimization, significantly accelerating the training process. The specific CUDA version used was 11.0, chosen for its compatibility with our PyTorch version (1.7.1). This combination ensured that we utilized the latest advancements in GPU technology, maximizing the efficiency of our model training processes.

The FACE (Fourier Analysis of Cross-Entropy) metric is a novel approach that applies Fourier analysis to the estimated cross-entropy of language. This method is particularly adept at measuring the similarity between model-generated and human-written languages, offering a unique perspective on the evaluation of natural language generation.

### Understanding the FACE Metric

FACE is based on empirical findings from psycholinguistics regarding the periodicity of entropy in language. It suggests that human language exhibits a periodic up-and-down pattern in entropy, which can be an important indicator of naturalness in generated text. By applying this concept, FACE attempts to quantitatively assess how closely the language generated by models aligns with these empirical findings of human language characteristics.

### Entropy Calculation Process

The initial step in the FACE evaluation involves computing the entropy of text data. This process requires analyzing the output of the models to estimate the cross-entropy, which is a measure of the difference between two probability distributions. In the context of language models, it typically represents the divergence between the model's predicted word distributions and the actual distributions in the target text.

### Fourier Transformation Application

Once the entropy sequences are obtained, Fourier analysis is applied to these sequences. This transformation converts the entropy data, which is initially in the time domain, into the frequency domain. This conversion is critical as it reveals the periodicity and other frequency-domain features of the entropy, which are central to the FACE metric.

### Interpreting FACE Scores

FACE provides scores that are designed to reflect various aspects of the text's quality and diversity, and how these align with human language characteristics. The scores include metrics like SO, CORR, SAM, and SPEAR, each representing different dimensions of the comparison between the entropy spectra of human and model-generated texts. Understanding these scores is key to interpreting the results of the FACE analysis, as they provide insights into how well the models' outputs mimic the natural entropy fluctuations found in human language.

